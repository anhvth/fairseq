{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5139957b-14b1-401b-9403-6ea6e795e142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c586d362-a2e7-4f12-9df8-b24bf406db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import data_utils\n",
    "from fairseq.data.dictionary import Dictionary\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "import argparse\n",
    "import pathlib\n",
    "\n",
    "import fairseq\n",
    "import torch\n",
    "from fairseq.models.roberta import RobertaModel as FairseqRobertaModel\n",
    "from fairseq.modules import TransformerSentenceEncoderLayer\n",
    "from packaging import version\n",
    "\n",
    "from transformers import RobertaConfig, AutoTokenizer\n",
    "import os\n",
    "if 0:#os.getenv(\"PRELAYERNORM\") is not None and os.getenv(\"PRELAYERNORM\") == \"1\":\n",
    "    print(\"Use pre layer norm\")\n",
    "    import sys\n",
    "    sys.path.append(\"/mnt/nfs-data/users/anhvth5/DinkyTrain\")\n",
    "    from huggingface.modeling_roberta_prelayernorm import RobertaForMaskedLM, RobertaForSequenceClassification\n",
    "    pre_layer_norm = True\n",
    "else:\n",
    "    from transformers import RobertaModel as TransformerRobertaModel\n",
    "    pre_layer_norm = False\n",
    "# from convert_dsfs_ckpt_to_fs_ckpt import convert_dsfs_ckpt_to_fs_ckpt\n",
    "from transformers.utils import logging\n",
    "import shutil\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af8dc7-afb0-4a1e-93ce-55b9c84b11d0",
   "metadata": {},
   "source": [
    "# Convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3ef736-8bf1-4c4b-a189-c5b636ed4001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_spm.model            checkpoint_177_105000.pt  checkpoint_48_30000.pt\n",
      "checkpoint_108_65000.pt   checkpoint_17_10000.pt    checkpoint_56_35000.pt\n",
      "checkpoint_116_70000.pt   checkpoint_185_110000.pt  checkpoint_73_45000.pt\n",
      "checkpoint_125_75000.pt   checkpoint_194_115000.pt  checkpoint_99_60000.pt\n",
      "checkpoint_134_80000.pt   checkpoint_203_120000.pt  checkpoint_9_5000.pt\n",
      "checkpoint_142_85000.pt   checkpoint_211_125000.pt  checkpoint_last.pt\n",
      "checkpoint_151_90000.pt   checkpoint_25_15000.pt    checkpoint_last.pt.tmp\n",
      "checkpoint_159_95000.pt   checkpoint_33_20000.pt    dict.txt\n",
      "checkpoint_168_100000.pt  checkpoint_40_25000.pt    test.py\n"
     ]
    }
   ],
   "source": [
    "ls ../store/vi-roberta-base/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d9041bb-2da2-471b-8995-4cafb47f7b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 09:26:12 | INFO | fairseq.file_utils | loading archive file ../store/vi-roberta-base/\n",
      "2023-10-09 09:26:13 | INFO | fairseq.tasks.masked_lm | dictionary: 98584 types\n",
      "2023-10-09 09:26:16 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:33433', 'distributed_port': 33433, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': True, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 125000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/workspace/fairseq/multirun/2023-10-08/10-44-31/0/checkpoints/checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'roberta', 'max_positions': 512, 'dropout': 0.1, 'attention_dropout': 0.1, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_dropout': 0.0, 'pooler_dropout': 0.0, 'max_source_positions': 512, 'no_token_positional_embeddings': False, 'encoder_learned_pos': True, 'layernorm_embedding': True, 'no_scale_embedding': True, 'activation_fn': 'gelu', 'encoder_normalize_before': False, 'pooler_activation_fn': 'tanh', 'untie_weights_roberta': False, 'adaptive_input': False, 'encoder_layerdrop': 0.0, 'encoder_layers_to_keep': None, 'quant_noise_pq': 0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0, 'spectral_norm_classification_head': False}, 'task': {'_name': 'masked_lm', 'data': '/mnt/nfs-data/users/anhvth5/store/vi-roberta-base', 'sample_break_mode': 'complete', 'tokens_per_sample': 510, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 125000.0, 'lr': [0.0005]}, 'scoring': None, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '../store/bert_spm.model', 'sentencepiece_enable_sampling': False, 'sentencepiece_alpha': None}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "roberta  = RobertaModel.from_pretrained('../store/vi-roberta-base/', 'checkpoint_211_125000.pt',bpe='sentencepiece', \n",
    "                                                sentencepiece_model='../store/bert_spm.model')\n",
    "roberta.eval();\n",
    "roberta_sent_encoder = roberta.model.encoder.sentence_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680edbd6-0913-4632-9e11-b206eb2fd4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cf98949-86be-43b3-acf9-f0cced2a526d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our RoBERTa config: RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 98585\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(98585, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=98585, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_config = \"roberta-base\"\n",
    "\n",
    "config = RobertaConfig.from_pretrained(hf_model_config)\n",
    "config.vocab_size = 98585\n",
    "if classification_head:\n",
    "    config.num_labels = roberta.model.classification_heads[\"mnli\"].out_proj.weight.shape[0]\n",
    "print(\"Our RoBERTa config:\", config)\n",
    "\n",
    "model = RobertaForSequenceClassification(config) if classification_head else RobertaForMaskedLM(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8df8701d-7b1e-401e-aa21-2496728baa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_layer_norm = False\n",
    "classification_head = False\n",
    "# Now let's copy all the weights.\n",
    "# Embeddings\n",
    "model.roberta.embeddings.word_embeddings.weight = roberta_sent_encoder.embed_tokens.weight\n",
    "model.roberta.embeddings.position_embeddings.weight = roberta_sent_encoder.embed_positions.weight\n",
    "model.roberta.embeddings.token_type_embeddings.weight.data = torch.zeros_like(\n",
    "    model.roberta.embeddings.token_type_embeddings.weight\n",
    ")  # just zero them out b/c RoBERTa doesn't use them.\n",
    "model.roberta.embeddings.LayerNorm.weight = roberta_sent_encoder.layernorm_embedding.weight\n",
    "model.roberta.embeddings.LayerNorm.bias = roberta_sent_encoder.layernorm_embedding.bias\n",
    "\n",
    "for i in range(config.num_hidden_layers):\n",
    "    # Encoder: start of layer\n",
    "    layer = model.roberta.encoder.layer[i]\n",
    "    roberta_layer: TransformerSentenceEncoderLayer = roberta_sent_encoder.layers[i]\n",
    "\n",
    "    # self attention\n",
    "    self_attn = layer.attention.self\n",
    "    assert (\n",
    "        roberta_layer.self_attn.k_proj.weight.data.shape\n",
    "        == roberta_layer.self_attn.q_proj.weight.data.shape\n",
    "        == roberta_layer.self_attn.v_proj.weight.data.shape\n",
    "        == torch.Size((config.hidden_size, config.hidden_size))\n",
    "    )\n",
    "\n",
    "    self_attn.query.weight.data = roberta_layer.self_attn.q_proj.weight\n",
    "    self_attn.query.bias.data = roberta_layer.self_attn.q_proj.bias\n",
    "    self_attn.key.weight.data = roberta_layer.self_attn.k_proj.weight\n",
    "    self_attn.key.bias.data = roberta_layer.self_attn.k_proj.bias\n",
    "    self_attn.value.weight.data = roberta_layer.self_attn.v_proj.weight\n",
    "    self_attn.value.bias.data = roberta_layer.self_attn.v_proj.bias\n",
    "\n",
    "    # self-attention output\n",
    "    self_output = layer.attention.output\n",
    "    assert self_output.dense.weight.shape == roberta_layer.self_attn.out_proj.weight.shape\n",
    "    self_output.dense.weight = roberta_layer.self_attn.out_proj.weight\n",
    "    self_output.dense.bias = roberta_layer.self_attn.out_proj.bias\n",
    "    if pre_layer_norm:\n",
    "        layer.attention.LayerNorm.weight = roberta_layer.self_attn_layer_norm.weight\n",
    "        layer.attention.LayerNorm.bias = roberta_layer.self_attn_layer_norm.bias\n",
    "    else:\n",
    "        self_output.LayerNorm.weight = roberta_layer.self_attn_layer_norm.weight\n",
    "        self_output.LayerNorm.bias = roberta_layer.self_attn_layer_norm.bias\n",
    "\n",
    "    # intermediate\n",
    "    intermediate = layer.intermediate\n",
    "    assert intermediate.dense.weight.shape == roberta_layer.fc1.weight.shape\n",
    "    intermediate.dense.weight = roberta_layer.fc1.weight\n",
    "    intermediate.dense.bias = roberta_layer.fc1.bias\n",
    "\n",
    "    # output\n",
    "    bert_output = layer.output\n",
    "    assert bert_output.dense.weight.shape == roberta_layer.fc2.weight.shape\n",
    "    bert_output.dense.weight = roberta_layer.fc2.weight\n",
    "    bert_output.dense.bias = roberta_layer.fc2.bias\n",
    "    if pre_layer_norm:\n",
    "        layer.intermediate.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n",
    "        layer.intermediate.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n",
    "    else:\n",
    "        bert_output.LayerNorm.weight = roberta_layer.final_layer_norm.weight\n",
    "        bert_output.LayerNorm.bias = roberta_layer.final_layer_norm.bias\n",
    "\n",
    "    # end of layer\n",
    "\n",
    "# The last layer norm layer for pre-layernorm  \n",
    "if pre_layer_norm:\n",
    "    model.roberta.LayerNorm.weight = roberta_sent_encoder.layer_norm.weight\n",
    "    model.roberta.LayerNorm.bias = roberta_sent_encoder.layer_norm.bias\n",
    "\n",
    "if classification_head:\n",
    "    model.classifier.dense.weight = roberta.model.classification_heads[\"mnli\"].dense.weight\n",
    "    model.classifier.dense.bias = roberta.model.classification_heads[\"mnli\"].dense.bias\n",
    "    model.classifier.out_proj.weight = roberta.model.classification_heads[\"mnli\"].out_proj.weight\n",
    "    model.classifier.out_proj.bias = roberta.model.classification_heads[\"mnli\"].out_proj.bias\n",
    "else:\n",
    "    # LM Head\n",
    "    model.lm_head.dense.weight = roberta.model.encoder.lm_head.dense.weight\n",
    "    model.lm_head.dense.bias = roberta.model.encoder.lm_head.dense.bias\n",
    "    model.lm_head.layer_norm.weight = roberta.model.encoder.lm_head.layer_norm.weight\n",
    "    model.lm_head.layer_norm.bias = roberta.model.encoder.lm_head.layer_norm.bias\n",
    "    model.lm_head.decoder.weight = roberta.model.encoder.lm_head.weight\n",
    "    model.lm_head.decoder.bias = roberta.model.encoder.lm_head.bias\n",
    "    model.lm_head.bias = roberta.model.encoder.lm_head.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73a85dda-1653-48ba-94a3-89524eddae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "557ec114-30f5-4daf-b68d-5fb4b787db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = 'xin ch맖'\n",
    "hf_ckpt_path = '.cache/hf_ckpt_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "275d2c70-230c-45ce-95b9-803b77fb1744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_absolute_diff = 2.09808349609375e-05\n",
      "Do both models output the same tensors? 游댠\n",
      "Saving model to .cache/hf_ckpt_path\n"
     ]
    }
   ],
   "source": [
    "# Let's check that we get the same results.\n",
    "input_ids: torch.Tensor = roberta.encode(SAMPLE_TEXT).unsqueeze(0)  # batch of size 1\n",
    "our_output = model(input_ids, output_hidden_states=True)\n",
    "our_output_final = our_output[0].cpu()\n",
    "our_output = our_output.hidden_states \n",
    "if classification_head:\n",
    "    their_output = roberta.model.classification_heads[\"mnli\"](roberta.extract_features(input_ids))\n",
    "else:\n",
    "    their_output = roberta.model(input_ids, return_all_hiddens=True)\n",
    "    their_output_final = their_output[0].cpu()\n",
    "    their_output = their_output[1]['inner_states'] \n",
    "\n",
    "# for i in range(len(our_output)):\n",
    "#     max_absolute_diff = torch.max(torch.abs(our_output[i].cpu() - their_output[i].transpose(0, 1).cpu())).item()\n",
    "#     mean_absolute_diff = torch.mean(torch.abs(our_output[i].cpu() - their_output[i].transpose(0, 1).cpu())).item()\n",
    "#     print(\"Layer %d max diff: %f mean diff: %f\" % (i, max_absolute_diff, mean_absolute_diff)) \n",
    "\n",
    "max_absolute_diff = torch.max(torch.abs(our_output_final - their_output_final)).item()\n",
    "print(f\"max_absolute_diff = {max_absolute_diff}\") \n",
    "success = torch.allclose(our_output_final, their_output_final, atol=1e-1)\n",
    "print(\"Do both models output the same tensors?\", \"游댠\" if success else \"游눨\")\n",
    "if not success:\n",
    "    raise Exception(\"Something went wRoNg\")\n",
    "pathlib.Path(hf_ckpt_path).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving model to {hf_ckpt_path}\")\n",
    "model.save_pretrained(hf_ckpt_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(hf_model_config)\n",
    "# tokenizer.save_pretrained(hf_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4a0cd58-b1e9-4543-8cc0-0885faa450fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at .cache/hf_ckpt_path/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "hf = AutoModel.from_pretrained('.cache/hf_ckpt_path/').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a4c886a-aeb2-4052-a9f3-6abbc9c5ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = roberta.encode('xin ch맖')\n",
    "r_features = roberta.extract_features(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b887a09d-2952-4fe1-a1dd-3ee2a9ba9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_features = hf.forward(ids[None]).last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9e29cdd-6419-4347-b577-5b50e1dc4cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 768]), torch.Size([1, 4, 768]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_features.shape, h_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e7a5a0a-078c-4b68-9ece-a347743a023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(r_features, h_features, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eb1e66b-465c-48bb-9086-28d40150c438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(r_features.mean(1), h_features.mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adf94769-b703-4409-b46e-54a856854620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xin cho m맟 m敲뗪 con ch칩', 0.6511508226394653, ' cho'),\n",
       " ('xin  m맟 m敲뗪 con ch칩', 0.025863606482744217, ' '),\n",
       " ('xin t故읉g m맟 m敲뗪 con ch칩', 0.02465791068971157, ' t故읉g'),\n",
       " ('xin ch맖 m맟 m敲뗪 con ch칩', 0.016041168943047523, ' ch맖'),\n",
       " ('xin d故몇 m맟 m敲뗪 con ch칩', 0.010530495084822178, ' d故몇')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.fill_mask('xin <mask> m맟 m敲뗪 con ch칩')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "47643c3a-1242-44df-9de7-4dca950ad975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, AutoModel\n",
    "import os.path as osp\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from fairseq.data.dictionary import Dictionary\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class KiBertTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self,vocab_file, spm, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert osp.exists(vocab_file), vocab_file\n",
    "        assert osp.exists(spm), spm\n",
    "        self.sp = SentencePieceProcessor(spm)\n",
    "        source_dict = Dictionary()\n",
    "        self.source_dict = source_dict.load(vocab_file)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path_to_pretrained):\n",
    "        return cls(osp.join(path_to_pretrained, 'dict.txt'), osp.join(path_to_pretrained, 'bert_spm.model'))\n",
    "    \n",
    "    def encode(self, sentence: str, *addl_sentences, no_separator=False, return_tensors='pt') -> torch.LongTensor:\n",
    "        sp, source_dictionary = self.sp, self.source_dict\n",
    "        if return_tensors != 'pt':\n",
    "            raise NotImplemented\n",
    "        def _encode(x:str):\n",
    "            return \" \".join(\n",
    "                        sp.Encode(\n",
    "                            x, out_type=str, enable_sampling=False, alpha=None,\n",
    "                        )\n",
    "                    )\n",
    "        bpe_sentence = \"<s> \" + _encode(sentence) + \" </s>\"\n",
    "        for s in addl_sentences:\n",
    "            bpe_sentence += \" </s>\" if not no_separator else \"\"\n",
    "            bpe_sentence += \" \" + _encode(s) + \" </s>\"\n",
    "        tokens = source_dictionary.encode_line(\n",
    "            bpe_sentence, append_eos=False, add_if_not_exist=False\n",
    "        )\n",
    "        return tokens.long()\n",
    "        \n",
    "    def decode(self, token_ids: torch.LongTensor) -> str:\n",
    "        token_strs = [self.source_dict[idx] for idx in token_ids]\n",
    "        return self.sp.DecodePieces(token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9febd515-2551-472a-a127-bfabc69e889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KiBertTokenizer.from_pretrained('../store/vi-roberta-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7afaae1e-f3c7-42f2-a6fe-ab91ac5ea5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode('erfsdfsadf asdf asdf asdf asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f85bcaf2-6532-4461-bd6c-b99a38fc23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "557d5a6e-1682-4399-a98e-8f8d9b675ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ht.encode('hello', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f06724d6-3f5a-4157-86d5-aced1248f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at .cache/hf_ckpt_path/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ea34a3a5-9ad0-434e-af64-a0578eae82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ht.encode('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cda91f7d-d723-4a99-ac21-48b6491ba7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at .cache/hf_ckpt_path/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape=torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('.cache/hf_ckpt_path/')\n",
    "ids = tokenizer.encode('xin')[None]\n",
    "features = model(ids).last_hidden_state\n",
    "print(f'{features.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c58ab-8336-4aab-bcf1-c50674b9c0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
